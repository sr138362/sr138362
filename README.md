HW03 â€“ Project Summary
- I chose Beautiful Soup over Scrapy because it is beginner-friendly and it integrates seamlessly with requests. It also allowed quicker iteration and testing using VS Code terminal. The documentation is easier to follow for rapid parsing of Wikipedia HTML tables. in total the installation took 2 minutes, locating the Table and parsing logic look about 30 minutes and Debugging dates and cell took about 45 minutes.

- I used ScrapeGraphAI to generate and run AI scraping graphs using a natural language prompt. the pront used was "extract all Falcon 9 Block 4 and Block 5 launch data from the table. Include engine number, flight number, launch date (in YYYY-MM-DD format), block type, launch pad, landing site, turnaround time (in days), engine status, and total launches. i used this because it would use keywords like "Falcon 9 Block 4 and Block 5" to limit irrelevant rows. I specified the exact column names from the table to ensure AI maps the structure correctly."in YYYY-MM-DD format" was used to convert inconsistent date formats automatically. The turnaround time was explicitly added to force it to parse the correct td cell, even if it had reference links or extra text.

- The Python development tools employed was in VS Code. First i built extraction scripts using BeautifulSoup to ensure I understood the table layout and data cells. Then i implement matching AI-based versions using ScrapeGraphAI and compare outputs. I also had to isolate the fastestTurnaround and longestTurnaround cases for Block 4 and 5 boosters only.

- The AI scraping via ScrapeGraphAI is powerful but very hard to work with as it requires exact prompt wording and correct setup. The BeautifulSoup was easier for low-level control, while ScrapeGraphAI felt more intuitive once properly configured. I now understand both traditional scraping and how LLMs can enhance automation when paired correctly.
